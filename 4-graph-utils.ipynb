{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "% run 0-utils.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import SpectralClustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grid(m):\n",
    "    idx = np.arange(m)\n",
    "    return np.reshape(np.meshgrid(idx, idx), (2, -1)).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def distance(z, k=4, metric='euclidean'):\n",
    "    d = spatial.distance.pdist(z, metric)\n",
    "    d = spatial.distance.squareform(d)\n",
    "    \n",
    "    width = d.mean() # preserve distribution\n",
    "    w = np.exp(- np.square(d / width))\n",
    "    np.fill_diagonal(w, 0)\n",
    "    \n",
    "    cut = w < np.sort(w, axis=1)[:, -k]\n",
    "    w[cut & np.transpose(cut)] = 0\n",
    "    \n",
    "    return w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fourier(laplacian):\n",
    "    return sp.linalg.svd(laplacian)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Temporary from\n",
    "# MichaÃ«l Defferrard, Xavier Bresson, Pierre Vandergheynst, \n",
    "# Convolutional Neural Networks on Graphs with Fast Localized Spectral Filtering, Neural Information Processing Systems (NIPS), 2016.\n",
    "\n",
    "def coarsen(A, levels, self_connections=False):\n",
    "    \"\"\"\n",
    "    Coarsen a graph, represented by its adjacency matrix A, at multiple\n",
    "    levels.\n",
    "    \"\"\"\n",
    "    graphs, parents = metis(A, levels)\n",
    "    perms = compute_perm(parents)\n",
    "\n",
    "    for i, A in enumerate(graphs):\n",
    "        M, M = A.shape\n",
    "\n",
    "        if not self_connections:\n",
    "            A = A.tocoo()\n",
    "            A.setdiag(0)\n",
    "\n",
    "        if i < levels:\n",
    "            A = perm_adjacency(A, perms[i])\n",
    "\n",
    "        A = A.tocsr()\n",
    "        A.eliminate_zeros()\n",
    "        graphs[i] = A\n",
    "\n",
    "        Mnew, Mnew = A.shape\n",
    "        print('Layer {0}: M_{0} = |V| = {1} nodes ({2} added),'\n",
    "              '|E| = {3} edges'.format(i, Mnew, Mnew-M, A.nnz//2))\n",
    "\n",
    "    return graphs, perms[0] if levels > 0 else None\n",
    "\n",
    "def coarsen_amg(A, levels, self_connections=False):\n",
    "    \"\"\"\n",
    "    Coarsen a graph, represented by its adjacency matrix A, at multiple\n",
    "    levels.\n",
    "    \"\"\"\n",
    "    graphs, parents = amg(A, levels)\n",
    "    perms = compute_perm_amg(parents)\n",
    "\n",
    "    for i, A in enumerate(graphs):\n",
    "        M, M = A.shape\n",
    "\n",
    "        if not self_connections:\n",
    "            A = A.tocoo()\n",
    "            A.setdiag(0)\n",
    "\n",
    "        if i < levels:\n",
    "            A = perm_adjacency(A, perms[i])\n",
    "\n",
    "        A = A.tocsr()\n",
    "        A.eliminate_zeros()\n",
    "        graphs[i] = A\n",
    "\n",
    "        Mnew, Mnew = A.shape\n",
    "        print('Layer {0}: M_{0} = |V| = {1} nodes ({2} added),'\n",
    "              '|E| = {3} edges'.format(i, Mnew, Mnew-M, A.nnz//2))\n",
    "\n",
    "    return graphs, perms[0] if levels > 0 else None\n",
    "\n",
    "\n",
    "def metis(W, levels, rid=None):\n",
    "    \"\"\"\n",
    "    Coarsen a graph multiple times using the METIS algorithm.\n",
    "    INPUT\n",
    "    W: symmetric sparse weight (adjacency) matrix\n",
    "    levels: the number of coarsened graphs\n",
    "    OUTPUT\n",
    "    graph[0]: original graph of size N_1\n",
    "    graph[2]: coarser graph of size N_2 < N_1\n",
    "    graph[levels]: coarsest graph of Size N_levels < ... < N_2 < N_1\n",
    "    parents[i] is a vector of size N_i with entries ranging from 1 to N_{i+1}\n",
    "        which indicate the parents in the coarser graph[i+1]\n",
    "    nd_sz{i} is a vector of size N_i that contains the size of the supernode in the graph{i}\n",
    "    NOTE\n",
    "    if \"graph\" is a list of length k, then \"parents\" will be a list of length k-1\n",
    "    \"\"\"\n",
    "\n",
    "    N, N = W.shape\n",
    "    if rid is None:\n",
    "        rid = np.random.permutation(range(N))\n",
    "    parents = []\n",
    "    degree = W.sum(axis=0) - W.diagonal()\n",
    "    graphs = []\n",
    "    graphs.append(W)\n",
    "    #supernode_size = np.ones(N)\n",
    "    #nd_sz = [supernode_size]\n",
    "    #count = 0\n",
    "\n",
    "    #while N > maxsize:\n",
    "    for _ in range(levels):\n",
    "\n",
    "        #count += 1\n",
    "\n",
    "        # CHOOSE THE WEIGHTS FOR THE PAIRING\n",
    "        # weights = ones(N,1)       # metis weights\n",
    "        weights = degree            # graclus weights\n",
    "        # weights = supernode_size  # other possibility\n",
    "        weights = np.array(weights).squeeze()\n",
    "\n",
    "        # PAIR THE VERTICES AND CONSTRUCT THE ROOT VECTOR\n",
    "        idx_row, idx_col, val = sp.sparse.find(W)\n",
    "        perm = np.argsort(idx_row)\n",
    "        rr = idx_row[perm]\n",
    "        cc = idx_col[perm]\n",
    "        vv = val[perm]\n",
    "        cluster_id = metis_one_level(rr,cc,vv,rid,weights)  # rr is ordered\n",
    "        parents.append(cluster_id)\n",
    "\n",
    "        # TO DO\n",
    "        # COMPUTE THE SIZE OF THE SUPERNODES AND THEIR DEGREE \n",
    "        #supernode_size = full(   sparse(cluster_id,  ones(N,1) , supernode_size )     )\n",
    "        #print(cluster_id)\n",
    "        #print(supernode_size)\n",
    "        #nd_sz{count+1}=supernode_size;\n",
    "\n",
    "        # COMPUTE THE EDGES WEIGHTS FOR THE NEW GRAPH\n",
    "        nrr = cluster_id[rr]\n",
    "        ncc = cluster_id[cc]\n",
    "        nvv = vv\n",
    "        Nnew = cluster_id.max() + 1\n",
    "        # CSR is more appropriate: row,val pairs appear multiple times\n",
    "        W = sp.sparse.csr_matrix((nvv,(nrr,ncc)), shape=(Nnew,Nnew))\n",
    "        W.eliminate_zeros()\n",
    "        # Add new graph to the list of all coarsened graphs\n",
    "        graphs.append(W)\n",
    "        N, N = W.shape\n",
    "\n",
    "        # COMPUTE THE DEGREE (OMIT OR NOT SELF LOOPS)\n",
    "        degree = W.sum(axis=0)\n",
    "        #degree = W.sum(axis=0) - W.diagonal()\n",
    "\n",
    "        # CHOOSE THE ORDER IN WHICH VERTICES WILL BE VISTED AT THE NEXT PASS\n",
    "        #[~, rid]=sort(ss);     # arthur strategy\n",
    "        #[~, rid]=sort(supernode_size);    #  thomas strategy\n",
    "        #rid=randperm(N);                  #  metis/graclus strategy\n",
    "        ss = np.array(W.sum(axis=0)).squeeze()\n",
    "        rid = np.argsort(ss)\n",
    "\n",
    "    return graphs, parents\n",
    "\n",
    "def amg(W, levels=2, rid=None):\n",
    "    \"\"\"\n",
    "    Coarsen a graph multiple times using the METIS algorithm.\n",
    "    INPUT\n",
    "    W: symmetric sparse weight (adjacency) matrix\n",
    "    levels: the number of coarsened graphs\n",
    "    OUTPUT\n",
    "    graph[0]: original graph of size N_1\n",
    "    graph[2]: coarser graph of size N_2 < N_1\n",
    "    graph[levels]: coarsest graph of Size N_levels < ... < N_2 < N_1\n",
    "    parents[i] is a vector of size N_i with entries ranging from 1 to N_{i+1}\n",
    "        which indicate the parents in the coarser graph[i+1]\n",
    "    nd_sz{i} is a vector of size N_i that contains the size of the supernode in the graph{i}\n",
    "    NOTE\n",
    "    if \"graph\" is a list of length k, then \"parents\" will be a list of length k-1\n",
    "    \"\"\"\n",
    "\n",
    "    N, N = W.shape\n",
    "    if rid is None:\n",
    "        rid = np.random.permutation(range(N))\n",
    "    parents = []\n",
    "    degree = W.sum(axis=0) - W.diagonal()\n",
    "    graphs = []\n",
    "    graphs.append(W)\n",
    "    #supernode_size = np.ones(N)\n",
    "    #nd_sz = [supernode_size]\n",
    "    #count = 0\n",
    "\n",
    "    #while N > maxsize:\n",
    "    for _ in range(levels):\n",
    "\n",
    "        #count += 1\n",
    "\n",
    "        # CHOOSE THE WEIGHTS FOR THE PAIRING\n",
    "        # weights = ones(N,1)       # metis weights\n",
    "        weights = degree            # graclus weights\n",
    "        # weights = supernode_size  # other possibility\n",
    "        weights = np.array(weights).squeeze()\n",
    "\n",
    "        # PAIR THE VERTICES AND CONSTRUCT THE ROOT VECTOR\n",
    "        idx_row, idx_col, val = sp.sparse.find(W)\n",
    "        perm = np.argsort(idx_row)\n",
    "        rr = idx_row[perm]\n",
    "        cc = idx_col[perm]\n",
    "        vv = val[perm]\n",
    "        \n",
    "        cluster = SpectralClustering(n_clusters=int(N/2.), eigen_solver='amg', affinity='precomputed', n_jobs=-1, n_init=1)\n",
    "        cluster_id = cluster.fit_predict(W)\n",
    "        #cluster_id = metis_one_level(rr,cc,vv,rid,weights)  # rr is ordered\n",
    "        parents.append(cluster_id)\n",
    "\n",
    "        # TO DO\n",
    "        # COMPUTE THE SIZE OF THE SUPERNODES AND THEIR DEGREE \n",
    "        #supernode_size = full(   sparse(cluster_id,  ones(N,1) , supernode_size )     )\n",
    "        #print(cluster_id)\n",
    "        #print(supernode_size)\n",
    "        #nd_sz{count+1}=supernode_size;\n",
    "\n",
    "        # COMPUTE THE EDGES WEIGHTS FOR THE NEW GRAPH\n",
    "        nrr = cluster_id[rr]\n",
    "        ncc = cluster_id[cc]\n",
    "        nvv = vv\n",
    "        Nnew = cluster_id.max() + 1\n",
    "        # CSR is more appropriate: row,val pairs appear multiple times\n",
    "        W = sp.sparse.csr_matrix((nvv,(nrr,ncc)), shape=(Nnew,Nnew))\n",
    "        W.eliminate_zeros()\n",
    "        # Add new graph to the list of all coarsened graphs\n",
    "        graphs.append(W)\n",
    "        N, N = W.shape\n",
    "\n",
    "        # COMPUTE THE DEGREE (OMIT OR NOT SELF LOOPS)\n",
    "        degree = W.sum(axis=0)\n",
    "        #degree = W.sum(axis=0) - W.diagonal()\n",
    "\n",
    "        # CHOOSE THE ORDER IN WHICH VERTICES WILL BE VISTED AT THE NEXT PASS\n",
    "        #[~, rid]=sort(ss);     # arthur strategy\n",
    "        #[~, rid]=sort(supernode_size);    #  thomas strategy\n",
    "        #rid=randperm(N);                  #  metis/graclus strategy\n",
    "        ss = np.array(W.sum(axis=0)).squeeze()\n",
    "        rid = np.argsort(ss)\n",
    "\n",
    "    return graphs, parents\n",
    "\n",
    "\n",
    "# Coarsen a graph given by rr,cc,vv.  rr is assumed to be ordered\n",
    "def metis_one_level(rr,cc,vv,rid,weights):\n",
    "\n",
    "    nnz = rr.shape[0]\n",
    "    N = rr[nnz-1] + 1\n",
    "\n",
    "    marked = np.zeros(N, np.bool)\n",
    "    rowstart = np.zeros(N, np.int32)\n",
    "    rowlength = np.zeros(N, np.int32)\n",
    "    cluster_id = np.zeros(N, np.int32)\n",
    "\n",
    "    oldval = rr[0]\n",
    "    count = 0\n",
    "    clustercount = 0\n",
    "\n",
    "    for ii in range(nnz):\n",
    "        rowlength[count] = rowlength[count] + 1\n",
    "        if rr[ii] > oldval:\n",
    "            oldval = rr[ii]\n",
    "            rowstart[count+1] = ii\n",
    "            count = count + 1\n",
    "\n",
    "    for ii in range(N):\n",
    "        tid = rid[ii]\n",
    "        if not marked[tid]:\n",
    "            wmax = 0.0\n",
    "            rs = rowstart[tid]\n",
    "            marked[tid] = True\n",
    "            bestneighbor = -1\n",
    "            for jj in range(rowlength[tid]):\n",
    "                nid = cc[rs+jj]\n",
    "                if marked[nid]:\n",
    "                    tval = 0.0\n",
    "                else:\n",
    "                    tval = vv[rs+jj] * (1.0/weights[tid] + 1.0/weights[nid])\n",
    "                if tval > wmax:\n",
    "                    wmax = tval\n",
    "                    bestneighbor = nid\n",
    "\n",
    "            cluster_id[tid] = clustercount\n",
    "\n",
    "            if bestneighbor > -1:\n",
    "                cluster_id[bestneighbor] = clustercount\n",
    "                marked[bestneighbor] = True\n",
    "\n",
    "            clustercount += 1\n",
    "\n",
    "    return cluster_id\n",
    "\n",
    "def compute_perm_amg(parents):\n",
    "    \"\"\"\n",
    "    Return a list of indices to reorder the adjacency and data matrices so\n",
    "    that the union of two neighbors from layer to layer forms a binary tree.\n",
    "    \"\"\"\n",
    "\n",
    "    # Order of last layer is random (chosen by the clustering algorithm).\n",
    "    indices = []\n",
    "    if len(parents) > 0:\n",
    "        M_last = max(parents[-1]) + 1\n",
    "        indices.append(list(range(M_last)))\n",
    "\n",
    "    for parent in parents[::-1]:\n",
    "        indices_layer = []\n",
    "        for i in indices[-1]:\n",
    "            indices_node = list(np.where(parent == i)[0])\n",
    "            indices_layer.extend(indices_node)\n",
    "        indices.append(indices_layer)\n",
    "\n",
    "    return indices[::-1]\n",
    "\n",
    "def compute_perm(parents):\n",
    "    \"\"\"\n",
    "    Return a list of indices to reorder the adjacency and data matrices so\n",
    "    that the union of two neighbors from layer to layer forms a binary tree.\n",
    "    \"\"\"\n",
    "\n",
    "    # Order of last layer is random (chosen by the clustering algorithm).\n",
    "    indices = []\n",
    "    if len(parents) > 0:\n",
    "        M_last = max(parents[-1]) + 1\n",
    "        indices.append(list(range(M_last)))\n",
    "\n",
    "    for parent in parents[::-1]:\n",
    "        #print('parent: {}'.format(parent))\n",
    "\n",
    "        # Fake nodes go after real ones.\n",
    "        pool_singeltons = len(parent)\n",
    "\n",
    "        indices_layer = []\n",
    "        for i in indices[-1]:\n",
    "            indices_node = list(np.where(parent == i)[0])\n",
    "            #assert 0 <= len(indices_node) <= 2\n",
    "            #print('indices_node: {}'.format(indices_node))\n",
    "\n",
    "            # Add a node to go with a singelton.\n",
    "\n",
    "            if len(indices_node) is 1:\n",
    "                indices_node.append(pool_singeltons)\n",
    "                pool_singeltons += 1\n",
    "                #print('new singelton: {}'.format(indices_node))\n",
    "            # Add two nodes as children of a singelton in the parent.\n",
    "            elif len(indices_node) is 0:\n",
    "                indices_node.append(pool_singeltons+0)\n",
    "                indices_node.append(pool_singeltons+1)\n",
    "                pool_singeltons += 2\n",
    "                #print('singelton childrens: {}'.format(indices_node))\n",
    "\n",
    "            indices_layer.extend(indices_node)\n",
    "        indices.append(indices_layer)\n",
    "\n",
    "    # Sanity checks.\n",
    " \n",
    "    for i,indices_layer in enumerate(indices):\n",
    "        M = M_last*2**i\n",
    "        # Reduction by 2 at each layer (binary tree).\n",
    "        assert len(indices[0] == M)\n",
    "        # The new ordering does not omit an indice.\n",
    "        assert sorted(indices_layer) == list(range(M))\n",
    "   \n",
    "    return indices[::-1]\n",
    "\n",
    "assert (compute_perm([np.array([4,1,1,2,2,3,0,0,3]),np.array([2,1,0,1,0])])\n",
    "    == [[3,4,0,9,1,2,5,8,6,7,10,11],[2,4,1,3,0,5],[0,1,2]])\n",
    "\n",
    "def perm_data(x, indices):\n",
    "    \"\"\"\n",
    "    Permute data matrix, i.e. exchange node ids,\n",
    "    so that binary unions form the clustering tree.\n",
    "    \"\"\"\n",
    "    if indices is None:\n",
    "        return x\n",
    "\n",
    "    N, M = x.shape\n",
    "    Mnew = len(indices)\n",
    "    assert Mnew >= M\n",
    "    xnew = np.empty((N, Mnew))\n",
    "    for i,j in enumerate(indices):\n",
    "        # Existing vertex, i.e. real data.\n",
    "        if j < M:\n",
    "            xnew[:,i] = x[:,j]\n",
    "        # Fake vertex because of singeltons.\n",
    "        # They will stay 0 so that max pooling chooses the singelton.\n",
    "        # Or -infty ?\n",
    "        else:\n",
    "            xnew[:,i] = np.zeros(N)\n",
    "    return xnew\n",
    "\n",
    "def perm_adjacency(A, indices):\n",
    "    \"\"\"\n",
    "    Permute adjacency matrix, i.e. exchange node ids,\n",
    "    so that binary unions form the clustering tree.\n",
    "    \"\"\"\n",
    "    if indices is None:\n",
    "        return A\n",
    "\n",
    "    M, M = A.shape\n",
    "    Mnew = len(indices)\n",
    "    assert Mnew >= M\n",
    "    A = A.tocoo()\n",
    "\n",
    "    # Add Mnew - M isolated vertices.\n",
    "    if Mnew > M:\n",
    "        rows = sp.sparse.coo_matrix((Mnew-M,    M), dtype=np.float32)\n",
    "        cols = sp.sparse.coo_matrix((Mnew, Mnew-M), dtype=np.float32)\n",
    "        A = sp.sparse.vstack([A, rows])\n",
    "        A = sp.sparse.hstack([A, cols])\n",
    "\n",
    "    # Permute the rows and the columns.\n",
    "    perm = np.argsort(indices)\n",
    "    A.row = np.array(perm)[A.row]\n",
    "    A.col = np.array(perm)[A.col]\n",
    "\n",
    "    # assert np.abs(A - A.T).mean() < 1e-9\n",
    "    assert type(A) is sp.sparse.coo.coo_matrix\n",
    "    return A"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
