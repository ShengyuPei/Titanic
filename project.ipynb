{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Titanic\n",
    "\n",
    "## Can we leverage deep learning on irregular domains to save lifes?\n",
    "\n",
    "---\n",
    "\n",
    "*Teo Stocco, Pierre-Alexandre Lee, Yves Lamonato, Charles Thiebaut*, [EPFL](https://epfl.ch)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Network Tour of Data Science](https://github.com/mdeff/ntds_2017) final project:<br>\n",
    "This notebook contains a detailed overview through the whole project with all essential parts. As this work required several attempts and exploration, only relevant parts are kept here. You can however access their individual and unguided research notebooks in the `lab` folder.<br>\n",
    "This project was **not shared** with any other class.\n",
    "\n",
    "[Binder access](https://mybinder.org/v2/gh/zifeo/Titanic/) | [nbviewer access](https://nbviewer.jupyter.org/github/zifeo/Titanic/blob/master/project.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Introduction\n",
    "\n",
    "2. Data source\n",
    "\n",
    "3. Preprocessing\n",
    "\n",
    "4. Graphs\n",
    "\n",
    "5. Models\n",
    "\n",
    "6. Evaluation\n",
    "\n",
    "7. Conclusion\n",
    "\n",
    "8. References"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 - Introduction\n",
    "\n",
    "Icebergs and ships do not get well along each other. To avoid dramatic events such as the one that happened a century ago, we aim at helping a noble quest: differentiating icebergs and ships based on radar data to see whether any\n",
    "iceberg is drifting away and might cross the road of a ship.\n",
    "\n",
    "<br>\n",
    "\n",
    "|Â© Statoil/C-CORE - Icebergs and ships examples|\n",
    "|-|\n",
    "|![](./img/statoil-ccore.png)|\n",
    "\n",
    "<br>\n",
    "\n",
    "This remote sensing measurements can be performed either by planes or by satellites. The second can provide radar information up to 14 time a day as in the case of [Sentinel-1](https://fr.wikipedia.org/wiki/Sentinel-1). The C-Band radar manages to capture data in numerous conditions (e.g. darkness, rain, cloud, fog, etc.) and measures the energy reflected back called backscatter (Torres et al, 2012). Those data can later be analyzed and used to clear out potential collision between icebergs and ships. \n",
    "\n",
    "Building on the top of recent advances in the field of signal processing on graphs (Schuman et al., 2013) and deep learning on irregular domains (Bronstein et al., 2017), we investigate the performance of standard machine learning methods and the relevance of graph based convolutional neural networks to perform binary classification in this specific case (layered data). The new method provide a convenient way of getting rotational invariance over the data (Defferrard et al., 2017) and set up a flexible framework for structured pooling. \n",
    "\n",
    "As the pooling operations require adequate aggregation by coarsening the graph between layers, we experiment how this framework can be exploited through various processes: Graclus multilevel algorithm and algebraic multigrid techniques. We further extend by comparing on different cases: grid graph, knn graph and wrapped-knn graph. Finally, we show that one can take advantage of graphs to defined structured pooling.\n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "% load_ext autoreload\n",
    "% autoreload 2\n",
    "% matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy as sp\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import networkx as nx\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import pandas as pd\n",
    "import sys\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bigger figure\n",
    "plt.rcParams['figure.figsize'] = 18, 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting seed for reproducability\n",
    "np.random.seed(0)\n",
    "torch.manual_seed(0);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scope to specfic gpu\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '1'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 - Data source\n",
    "\n",
    "The dataset is provided by Statoil, an oil and gas compagny, and C-CORE, a monitoring company using computer vision to keep naval operations safe and efficient. It was released on Kaggle for prediction competition in late 2017. The full dataset contains `10'028` iceberg or ship cases with only `1'604` labelled. Some of the test images were computer generated to avoid hand labelling in the competition. As we will only focus on labelled one, this should not matter. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Description\n",
    "\n",
    "For each case, the following covariates are provided. Two radar bands of a resolution `75x75` corresponding to  \"transmitted and received horizontally\" (HH) and \"transmitted horizontally and received vertically\" (HV) data. One additional feature (angle) and the label.\n",
    "\n",
    "<br>\n",
    "\n",
    "| Feature | Description | Type | Has N/A | Comment |\n",
    "| - | - | - | - | - |\n",
    "| id | image identifier| String | No | |\n",
    "| band_1 | horizontal plane | Float array | No| HH |\n",
    "| band_2 | vertical plane | Float array | No| HV |\n",
    "| inc_angle | measurement angle | Float| Yes (~10%) | Unit in degrees |\n",
    "| is_iceberg | iceberg or not| Boolean (0/1)| No | Label |\n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gcnn.datasets import load_icebergs\n",
    "\n",
    "# train = all labelled cases from Kaggle\n",
    "measures = load_icebergs('train')\n",
    "measures.head(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "measures.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploration\n",
    "\n",
    "To get a first qualitative insight, let's have a look at an example of an iceberg."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_iceberg = measures.iloc[5]\n",
    "example_iceberg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gcnn import viz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "viz.plot_bands(example_iceberg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "viz.plot_bands_3d(example_iceberg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some remarks:\n",
    "- data seems to be centered\n",
    "- this example is round, however checking further cases reveal that they vary a lot in size and shape\n",
    "- the two bands can have a noticeable difference in intensity\n",
    "- noise presence\n",
    "\n",
    "---\n",
    "\n",
    "Let's now look at an example of a ship."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_ship = measures.iloc[0]\n",
    "example_ship"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "viz.plot_bands(example_ship)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "viz.plot_bands_3d(example_ship, angle=60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some remarks:\n",
    "\n",
    "- data also seems to be centered\n",
    "- this example has a ship-like shape, however checking further cases reveal that they also vary a lot\n",
    "- noise does not seem to be different\n",
    "\n",
    "---\n",
    "\n",
    "What about the label distribution?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.title('iceberg distribution')\n",
    "measures.groupby(measures.is_iceberg).is_iceberg.count().plot.barh();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The distribution of the two classes across the data is quite even (~53% of ships, ~47% of icebergs).\n",
    "\n",
    "---\n",
    "\n",
    "And what about distribution of features?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.count_nonzero(measures.inc_angle.isnull())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.title(\"Distribution of angles from the bands\")\n",
    "sns.distplot(measures.inc_angle.dropna());"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "133 of the measures have an absent angle, so we just ignore them for the correlation coefficient and they will be replaced later during the learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.title('Iceberg bands distribution')\n",
    "sns.distplot(example_iceberg.band_1, label='band 1')\n",
    "sns.distplot(example_iceberg.band_2, label='band 2')\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.title('Ship bands distribution')\n",
    "sns.distplot(example_ship.band_1, label='band 1')\n",
    "sns.distplot(example_ship.band_2, label='band 2')\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bands appear as bell-shapes and their difference might contain valuable information. This is why the same scaler will be used later to perserve their eventual gaps.\n",
    "\n",
    "---\n",
    "\n",
    "T-SNE allows to represent non-linear high dimensionality data on only two dimensions which can be easily visualized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pca can speed up T-SNE and suppress some residual noise\n",
    "pca50 = PCA(n_components=50).fit_transform(np.c_[np.stack(measures.band_1), np.stack(measures.band_2)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tsne = TSNE(n_components=2, perplexity=60).fit_transform(pca50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.title('T-NSE icebergs vs ships')\n",
    "plt.scatter(tsne[measures.is_iceberg == 1, 0], tsne[measures.is_iceberg == 1, 1], label='icebergs')\n",
    "plt.scatter(tsne[measures.is_iceberg == 0, 0], tsne[measures.is_iceberg == 0, 1], label='ships')\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The T-SNE does not show any obvious underlying clue (e.g. clusters) after some parameters tweaking. This problem is thus unlikely to be resolved by simple classifiers as such as k-nearest neighbors. It can be noticed that some regions are \"closely\" shared among icebergs and ships whereas other are more distinct."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prototypes\n",
    "\n",
    "One interesting first point is to look whether there are some prototypes (distinct primitive shapes). This allows to gather insights that will be later useful if there is any imbalance between icebergs and ships. For example, one could try to cluster similar average band together:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_proto = 8\n",
    "kmeans = KMeans(n_clusters=n_proto).fit(\n",
    "    (np.stack(measures.band_1) + np.stack(measures.band_2)) / 2\n",
    ")\n",
    "kmeans_centers = kmeans.cluster_centers_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, center in enumerate(kmeans_centers):\n",
    "    plt.subplot(1, 4, i % 4 + 1)\n",
    "    plt.imshow(center.reshape(75, 75))\n",
    "    if i % 4 == 3:\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# per cluster, per label, counts\n",
    "measures.is_iceberg.groupby(kmeans.labels_).apply(pd.value_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although the first two clusters seems to have a majority of ships, prototypes are unlikely to be specific to a given label in such high dimensions (partly because of curse of dimensionality). However this confirms the hypothesis on centered data and shows the prescence of some scatter reflections. Increasing the number of clusters looks to improve the classification between the two, however the quality does not improve. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 - Preprocessing\n",
    "\n",
    "There is no particular pre-processing to do (except from scaling and managing the N/A), since the data is already nicely formatted. Also since the image are already quite small (75x75), in particular the central zone of interest, we choose not to smooth it (with a gaussian filter by example) in order not to loose important details. Noise might thus have a strong impact but this will also allow to see how graph based learning manage that issue."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train and test splits\n",
    "\n",
    "Using the same `random_state`, the data is split the same way here as in the others notebooks where we tuned the models using cross-validation on the following train split. The test split is only used to compare final models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# random state is very important as they same split can be used in other notebooks\n",
    "train, test = train_test_split(range(len(measures)), test_size=0.15, stratify=measures.is_iceberg, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check that we did a fair (stratified) split for training and testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "measures.iloc[train].is_iceberg.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "measures.iloc[test].is_iceberg.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning and scaling\n",
    "\n",
    "The angle is likely to play an important role and will be replaced by `0` to stay distinct from known values. The bands will be scaled between 0 and 1 so that they preserve their respective mean (location)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "band_scaler = MinMaxScaler()\n",
    "angle_scaler = MinMaxScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for _, e in measures.iloc[train].iterrows():\n",
    "    band_scaler.partial_fit(e.band_1.reshape(1, -1))\n",
    "    band_scaler.partial_fit(e.band_2.reshape(1, -1))\n",
    "    \n",
    "angle_scaler.fit(measures.iloc[train].inc_angle.dropna().values.reshape(-1, 1));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Usable data contains the two bands, the angle and the target (label)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bands = np.stack([\n",
    "    band_scaler.transform(np.stack(measures.band_1)),\n",
    "    band_scaler.transform(np.stack(measures.band_2)),\n",
    "], axis=1).reshape(-1, 2, 75, 75)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "angles = angle_scaler.transform(measures.inc_angle.fillna(0).values.reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "targets = measures.is_iceberg.values.reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 - Graphs\n",
    "\n",
    "To prepare later graph-based learning, various grids and coarsening methods are showcased on small graphs. The motivation for using this dataset is to see how the two bands can be modelled as irregular domains versus classical convolution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gcnn import graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Classical 2D grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "small_grid = nx.grid_graph([5, 5])\n",
    "nx.draw(small_grid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Knn 2D grid\n",
    "\n",
    "Each node is connected to its k-nearest neighbors (or more in case of equality, this is due to the interpolation)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "small_knn = graph.knn(graph.grid_coordinates(5), k=8, metric='cityblock')\n",
    "nx.draw(small_knn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Wrap-around Knn 2D grid\n",
    "\n",
    "Same as above, with wrap-around borders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "small_wraps = graph.kwraps(5, kd=1)\n",
    "nx.draw(small_wraps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Classical 3D grid\n",
    "\n",
    "With two levels of depth."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "small_grid3d = nx.grid_graph([5, 5, 2])\n",
    "nx.draw(small_grid3d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Knn 3D grid\n",
    "\n",
    "With two levels of depth."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "small_knn3d = graph.knn3d(graph.grid_coordinates(5), k=8, metric='cityblock', d=2)\n",
    "nx.draw(small_knn3d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3D wrap-around grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "small_wraps3d = graph.kwraps3d(5, kd=1, d=2)\n",
    "nx.draw(small_wraps3d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's analyze how those graph compare in quantitative terms (e.g. connectivity)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "small_graphs = [small_grid, small_knn, small_wraps, small_grid3d, small_knn3d, small_wraps3d]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "[nx.average_node_connectivity(g) for g in small_graphs]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The first one (corresponding to the grid) should have an expected average around 4, as each node is connected to its four neighbors.\n",
    "- The second one corresponds to a KNN with K = 8, we should have an average around 8 although as it is a small graph the high number of \"corners\" increases this average.\n",
    "- The third one (wrapped grid) eliminates those border/corner cases and has the \"true\" average value. \n",
    "- The fourth one is similar to the 2D grid with one more dimension, therefore close to 4 as well.\n",
    "- The fifth one, as the 2D KNN, has a lot of corner cases that increases average. \n",
    "- The sixth (3D wrappred grid) that has 9 neighbors in the 2D plane plus 8 from the nodes above or below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Small grid : \", nx.average_clustering(small_grid))\n",
    "print(\"Small KNN : \", nx.average_clustering(small_knn))\n",
    "print(\"Small wraps : \", nx.average_clustering(small_wraps))\n",
    "print(\"Small grid 3D : \", nx.average_clustering(small_grid3d))\n",
    "print(\"Small KNN 3D : \", nx.average_clustering(small_knn3d))\n",
    "print(\"Small wraps 3D : \", nx.average_clustering(small_wraps3d))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When looking at the average clustering, one can underlying the two perfect grid cases when no asymmetry is present and thus no cluster. The other show an average clustering of ~0.5. However this does not tell much as the grid are small."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Coarsening"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The coarsening steps can be visualized step by step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Graclus\n",
    "\n",
    "[Graclus](http://www.cs.utexas.edu/users/dml/Software/graclus.html) is a fast graph clustering software that computes normalized or ratio cut. It might add some nodes to match a reduction by 2. Implementation is courtesy of Michael Defferrard."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gcnn.coarsening import graclus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graclus_levels, _ = graclus.coarsen(nx.adjacency_matrix(small_grid), levels=2, self_connections=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "viz.plot_graph_steps(graclus_levels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Algebraic multigrid\n",
    "\n",
    "Algebraic multigrid for graph coarsening is a method projecting signals to a coarser graph representation obtained via greedy selections of vertices. The implementation uses `sklearn.clustering.SpectralClustering` which runs [PyAMG](https://github.com/pyamg/pyamg) underneath."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gcnn.coarsening import amg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "small_grid = nx.grid_graph([4, 4, 1])\n",
    "small_dist = nx.adjacency_matrix(small_grid)\n",
    "graphs, perm = amg.coarsen(small_dist, levels=3, self_connections=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for g in graphs:\n",
    "    plt.subplot(121)\n",
    "    plt.spy(g.todense())\n",
    "    plt.subplot(122)\n",
    "    nx.draw(nx.from_numpy_array(g.todense()))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Kron reduction\n",
    "The Kron reduction of a graph is again a graph whose Laplacian matrix is obtained by the Schur complement of the original Laplacian matrix with respect to a subset of nodes. Implementation adapted from [PyGSP](http://pygsp.readthedocs.io/en/stable/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gcnn.coarsening import kron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kron_levels = kron.graph_multiresolution(sp.sparse.csr_matrix(nx.adjacency_matrix(small_grid)), levels=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for g in kron_levels:\n",
    "    g.set_coordinates()\n",
    "    g.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Maximum spanning tree\n",
    "\n",
    "We tried to use a MST (Maximum Spaning Tree) based on downsampling. The idea is to find the max-cut to separate our nodes in a bipartite graph, but this problem is in general NP-hard. So to find a high-cut in a fast way, we took some inspiration of this paper (Nguyen, \"Downsampling of Signals on Graphs Via Maximum Spanning Trees\") and implemented our own solution.\n",
    "\n",
    "The algorithm goes this way : find the MST of our connected (by construction) graph, using Prim's algorithm for example, then randomly choose one node to be the root node, and from there compute the distance of each node to the root node in the MST. From there, only keep the nodes which distance to the root is even. In the resulting graph, the weight between the nodes we kept are computed from the two edges that originally connected them in the MST.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gcnn.coarsening import mst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mst_levels = mst.mst(nx.adjacency_matrix(small_grid).todense(), levels=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "viz.plot_graph_steps(mst_levels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, we did not have the time to find an efficient way of knowing which nodes are being clustered together for the minimum spanning tree and kron based methods. This is left for future work."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5 - Models\n",
    "\n",
    "Starting from standards models, classical convolution will be tuned and serve as a reference against graph convolutions. All parameters were tuned in other notebooks on same training set (cross-validated) and reported here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = pd.DataFrame([], columns=['name', 'accuracy', 'precision', 'recall', 'f1']).set_index('name')\n",
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "flat_features = np.c_[bands.reshape(-1, 2 * 75 * 75), angles.reshape(-1, 1)]\n",
    "flat_features.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Standard methods\n",
    "\n",
    "Let's start with a dummy classifier, then try the basic k-nearest neighbors and finally evaluate against a logistic regression which should outperform the first two."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.dummy import DummyClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy = DummyClassifier().fit(flat_features[train], targets[train].ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gcnn.utils import score_classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores.loc['baseline'] = score_classification(targets[test], dummy.predict(flat_features[test]))\n",
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn = KNeighborsClassifier(n_neighbors=6).fit(flat_features[train], targets[train].ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores.loc['knn'] = score_classification(targets[test], knn.predict(flat_features[test]))\n",
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logistic = LogisticRegression(C=0.1).fit(flat_features[train], targets[train].ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores.loc['logistic'] = score_classification(targets[test], logistic.predict(flat_features[test]))\n",
    "scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convolution baseline\n",
    "\n",
    "Inspired from LeNet-5, the following architecture is used as a baseline for convolution.\n",
    "\n",
    "@INSERT LAYERS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cuda = torch.cuda.is_available()\n",
    "cuda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.utils.data as data_utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare pytorch tensors for later processing (cpu or gpu)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bands_t = torch.from_numpy(bands).float()\n",
    "angles_t = torch.from_numpy(angles).float()\n",
    "targets_t = torch.from_numpy(targets).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gcnn.nets import BaselineCNN\n",
    "from skorch import NeuralNet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Skorch provides a sklearn interface over pytorch models and will ease the training and evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn = NeuralNet(\n",
    "    BaselineCNN,\n",
    "    use_cuda=cuda,\n",
    "    optimizer=torch.optim.Adam,\n",
    "    criterion=nn.BCEWithLogitsLoss, \n",
    "    batch_size=50,\n",
    "    max_epochs=15,\n",
    "    lr=1e-4,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gcnn.utils import sigmoid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def score_fit_predict(name, model, bands, angles, targets):\n",
    "    \n",
    "    # fit model with bands and angles\n",
    "    train_features = dict(x=bands[train], x2=angles[train])\n",
    "    model.fit(train_features, targets[train])\n",
    "    print()\n",
    "    \n",
    "    # need logit for prediction as it is included within optimizer\n",
    "    test_features = dict(x=bands[test], x2=angles[test])\n",
    "    preds = model.predict_proba(test_features)\n",
    "    preds = sigmoid(preds).round()\n",
    "    \n",
    "    # save score\n",
    "    scores.loc[name] = score_classification(targets[test], preds)\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_fit_predict('conv', cnn, bands_t, angles_t, targets_t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Graph convolution (Graclus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Graph convolution was first based on graph fourier transform. However with large data, the cost of performing a change of basis becomes prohibitive. The alternative is to use a Cheyshev polynomial to avoid these computating and estimate the filtering. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.sparse\n",
    "from scipy.sparse import csgraph\n",
    "from gcnn.nets import GraphCNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def coarsen_permute(graph, bands, algo=graclus):\n",
    "    \n",
    "    # node should be ordered in a way corresponding to bands data order\n",
    "    if type(graph) != np.ndarray:\n",
    "        grid = nx.adjacency_matrix(graph, sorted(graph.nodes))\n",
    "    else:\n",
    "        grid = graph\n",
    "        \n",
    "    # coarsen grid\n",
    "    grid = sp.sparse.csr.csr_matrix(grid)\n",
    "    laps, perms = algo.coarsen(grid, levels=3, self_connections=False)\n",
    "    print()\n",
    "    \n",
    "    # compute normed laplacian\n",
    "    laps = [csgraph.laplacian(g, normed=True) for g in laps[:-1]]\n",
    "    \n",
    "    # update data accordingly\n",
    "    pbands = graclus.perm_data(bands, perms)\n",
    "    pbands = torch.from_numpy(pbands).float()\n",
    "    \n",
    "    return laps, pbands"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gc_model(laps):\n",
    "    l0 = torch.from_numpy(laps[0].todense()).float()\n",
    "    l2 = torch.from_numpy(laps[2].todense()).float()\n",
    "    \n",
    "    if cuda:\n",
    "        l0 = l0.cuda()\n",
    "        l2 = l2.cuda()\n",
    "    \n",
    "    return NeuralNet(\n",
    "        GraphCNN,\n",
    "        module__k=25, # if k None then fourier mode and should send fourier basis instead of laplacian\n",
    "        module__lf0=l0,\n",
    "        module__lf2=l2,\n",
    "        use_cuda=cuda,\n",
    "        optimizer=torch.optim.Adam,\n",
    "        criterion=nn.BCEWithLogitsLoss, \n",
    "        batch_size=50,\n",
    "        max_epochs=15,\n",
    "        lr=1e-4,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gc_single_band(name, grid, size=75, algo=graclus):\n",
    "    laps, pbands = coarsen_permute(grid, bands[:, 0, :, :].reshape(-1, size * size), algo)\n",
    "    return score_fit_predict(name, gc_model(laps), pbands, angles_t, targets_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gc_both_bands(name, grid, size=75, algo=graclus):\n",
    "    laps, pbands = coarsen_permute(grid, bands.reshape(-1, 2 * size * size), algo)\n",
    "    return score_fit_predict(name, gc_model(laps), pbands, angles_t, targets_t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As each graph has a particular nodes ordering, precaution need to be taken. First using the 2D graph, the learning will only happen on the first band."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gc_single_band('gcnn_grid', nx.grid_graph([75, 75]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gc_single_band('gcnn_knn', graph.knn(graph.grid_coordinates(75), k=8, metric='cityblock'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "gc_single_band('gcnn_kwraps', graph.kwraps(75, kd=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's take into account the two bands and use the 3D graphs. As some of the 3D graph have a large number of edges, some of them are randomly pruned to reduce required memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gc_both_bands('gcnn_grid_3', nx.grid_graph([75, 75, 2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gcnn.graph import remove_random_edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gc_both_bands('gcnn_knn_3', graph.knn3d(graph.grid_coordinates(75), k=4, metric='cityblock', d=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# too time-consuming\n",
    "# gc_both_bands(\n",
    "#     'gcnn_kwraps_3', \n",
    "#     remove_random_edges(\n",
    "#         graph.kwraps3d(\n",
    "#             75, \n",
    "#             kd=1, \n",
    "#             d=2\n",
    "#         ),\n",
    "#         50000\n",
    "#     )\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6 - Evaluation\n",
    "\n",
    "@qqun graph des rÃ©sultats\n",
    "\n",
    "et interprÃ¨te les rÃ©sultats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this table we have the different results for all the methods we tried. The baseline was just a random model, and we can clearly see that every score is around 0.5. For the KNN, the score is quite high, especially the recall, which is desirable for our application (classifying some chips as icebergs isn't so problematic, but we don't want to miss any iceberg).\n",
    "\n",
    "---\n",
    "\n",
    "The last model is by far our best, with good precision and accuracy, but an even better recall. The f1 score of the different models is a good summary of this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7 - Conclusion\n",
    "\n",
    "\n",
    "Even though can be further trained, we fixed to better compare and analyze\n",
    "\n",
    "Low data, what if more data ?\n",
    "hard to converge\n",
    "need new abstraction or library as operation can quickly become very complex (transforming data into grid on correct points)\n",
    "\n",
    "\n",
    "### Improvements\n",
    "\n",
    "- sparse operations\n",
    "- more parameters setting with std in scores reported\n",
    "- speedup and complexity analysis\n",
    "- graph deconv (view)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8 - References\n",
    "\n",
    "- TORRES, Ramon, SNOEIJ, Paul, GEUDTNER, Dirk, et al. GMES Sentinel-1 mission. Remote Sensing of Environment, 2012, vol. 120, p. 9-24.\n",
    "- SHUMAN, David I., NARANG, Sunil K., FROSSARD, Pascal, et al. The emerging field of signal processing on graphs: Extending high-dimensional data analysis to networks and other irregular domains. IEEE Signal Processing Magazine, 2013, vol. 30, no 3, p. 83-98.\n",
    "- BRONSTEIN, Michael M., BRUNA, Joan, LECUN, Yann, et al. Geometric deep learning: going beyond euclidean data. IEEE Signal Processing Magazine, 2017, vol. 34, no 4, p. 18-42.\n",
    "- DEFFERRARD, MichaÃ«l, BRESSON, Xavier, et VANDERGHEYNST, Pierre. Convolutional neural networks on graphs with fast localized spectral filtering. In : Advances in Neural Information Processing Systems. 2016. p. 3844-3852.\n",
    "- NGUYEN Ha Q., DO Minh N, et al. Downsampling of Signal on Graphs Via Maximum Spanning Trees. IEEE Transactions on Signal Processing, 2015, vol. 63, no 1.\n",
    "- DORFLER Florain, BULLO Francesco. Kron reduction of graphs with applications to electrical networks. 2011."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
