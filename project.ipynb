{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Titanic\n",
    "\n",
    "## Can we leverage deep learning on irregular domains to save lifes?\n",
    "\n",
    "---\n",
    "\n",
    "*Teo Stocco, Pierre-Alexandre Lee, Yves Lamonato, Charles Thiebaut*, [EPFL](https://epfl.ch)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Network Tour of Data Science](https://github.com/mdeff/ntds_2017) final project. This notebook contains a detailed overview through the whole project with all essential parts. As this work required several attempts and exploration, only relevant parts are kept here. You can however access their associated notebooks (unguided) with all processes when mentionned. This project was **not shared** with any other class.\n",
    "\n",
    "\n",
    "[Binder access](https://mybinder.org/v2/gh/zifeo/Titanic/) | [nbviewer access](https://nbviewer.jupyter.org/github/zifeo/Titanic/blob/master/project.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO:\n",
    "\n",
    "- spell check plugin\n",
    "- isolate code blocks in functions in separate Python module\n",
    "- abstract\n",
    "- notebook toc plugin\n",
    "- README with getting started"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 Introduction\n",
    "\n",
    "Icebergs and ships do not get well along each other. To avoid dramatic events such as the one that happened a century ago, we aim at helping a noble quest: differentiating icebergs and ships based on radar data to see whether any\n",
    "iceberg is drifting away and might cross the road of a ship.\n",
    "\n",
    "|Â© Statoil/C-CORE - Icebergs and ships examples|\n",
    "|-|\n",
    "|![](./img/statoil-ccore.png)|\n",
    "\n",
    "This remote sensing measurements can be performed either by planes or by satellites. The latter can provide radar information up to 14 time a day as in the case of [Sentinel-1](https://fr.wikipedia.org/wiki/Sentinel-1). The C-Band radar manage to capture data in numerous conditions (e.g. darkness, rain, cloud, fog, etc.) and measures the energy reflected back called backscatter (Torres et al, 2012). Those data can latter be analyzed and used to clear out potential collision between icebergs and ships. \n",
    "\n",
    "Building on the top of recent advances in the field of signal processing on graphs (Schuman et al., 2013) and deep learning on irregular domains (Bronstein et al., 2017), we investigate the performance of standard machine learning methods and the relevance of graph based convolutional neural networks to perform binary classification in this specific case. The new method provide a convenient way of getting rotational invariance over the data and set up a flexible framework for structured pooling (Defferrard et al., 2017). Pooling operations require adequate aggregation by coarsening the graph between layers. We experiment how this framework can be exploited through various processes: Graclus multilevel algorithm, Kron reduction, algebraic multigrid techniques and via maximum spanning tree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "% load_ext autoreload\n",
    "% autoreload 2\n",
    "% matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy as sp\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import networkx as nx\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import pandas as pd\n",
    "import sys\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add gcnn module\n",
    "sys.path.append('..')\n",
    "\n",
    "# bigger figure\n",
    "plt.rcParams['figure.figsize'] = 18, 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting seed for reproducability\n",
    "np.random.seed(0)\n",
    "torch.manual_seed(0);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scope to specfic gpu\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '1'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 Data source\n",
    "\n",
    "The dataset is provided by Statoil, an oil and gas compagny, and C-CORE, a monitoring company using compute vision, to keep operations safe and efficient. It was released on Kaggle for prediction competition in late 2017. The full dataset contains 10'028 iceberg or ship cases with only 1'604 labelled. Some of the images were computer generated to avoid hand labelling in the competition. As we will only focus on labelled one, this should not matter. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Description\n",
    "\n",
    "- HH: transmit and receive horizontally\n",
    "- HV: transmit horizontally and receive vertically\n",
    "\n",
    "Quantitative vs Qualitative\n",
    "\n",
    "\n",
    "| Feature | Description | Type | Has N/A | Comment |\n",
    "| - | - | - | - | - |\n",
    "| id | image identifier| String | No | |\n",
    "| band_1 | horizontal plane | Float array | No| |\n",
    "| band_2 | vertical plane | Float array | No|  |\n",
    "| inc_angle | measurement angle | Float| Yes(~10%) | Unit in degrees |\n",
    "| is_iceberg | label (iceberg or not)| Boolean (0/1)| No| |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gcnn.datasets import load_icebergs\n",
    "\n",
    "measures = load_icebergs('train')\n",
    "measures.head(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "measures.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploration\n",
    "\n",
    "> More details on associated notebook [xx]()\n",
    "\n",
    "some more cases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's have a look at an example of iceberg."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_iceberg = measures.iloc[5]\n",
    "example_iceberg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gcnn import viz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "viz.plot_bands(example_iceberg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "viz.plot_bands_3d(example_iceberg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- varies a lot in size\n",
    "- \"intensity\" difference\n",
    "- lots of backscatter noise\n",
    "\n",
    "---\n",
    "\n",
    "Let's now look at an example of a ship."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_ship = measures.iloc[0]\n",
    "example_ship"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "viz.plot_bands(example_ship)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "viz.plot_bands_3d(example_ship, angle=60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- some elongated shape\n",
    "- other angle but not visible from eye\n",
    "- still lots of noise\n",
    "\n",
    "---\n",
    "\n",
    "What about the distribution of the data and correlation ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.title('iceberg distribution')\n",
    "measures.groupby(measures.is_iceberg).is_iceberg.count().plot.barh();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The distribution of the two classes across the data is quite even."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "distribution of bands and angle? covariance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.title(\"Distribution of angles from the bands\")\n",
    "measures.inc_angle.plot.hist(bins=30);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "133 of the measures have an absent angle, so we just ignore them for the correlation coefficient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "measures_clean = measures[measures.inc_angle.notnull()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see below that there seem no to be any apparent correlation between the angle of the measure and the label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.corrcoef(measures_clean.inc_angle, measures_clean.is_iceberg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TSNE allows to represent high dimensionality data on only two or three dimensions which can be easily visualized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=50).fit_transform(np.c_[np.stack(measures.band_1), np.stack(measures.band_2)])\n",
    "# pca speed up T-SNE and suppress some noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tsne = TSNE(n_components=2, perplexity=60).fit_transform(pca)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(tsne[measures.is_iceberg == 1, 0], tsne[measures.is_iceberg == 1, 1], label='icebergs')\n",
    "plt.scatter(tsne[measures.is_iceberg == 0, 0], tsne[measures.is_iceberg == 0, 1], label='ships')\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The TSNE doesn't show any clusters even when we tweak the hyperparameters. This result is expected, otherwise we would be able to solve the problem easily using a simple classifier such as K-means on the output of TSNE."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prototypes\n",
    "\n",
    "One interesting first point is to look whether there are some prototypes (distinct primitive shapes). This allows to gather insights that will be later useful if there is any unbalanced between icebergs and ships. For example, one could try to cluster similar band 1 together:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_proto = 4\n",
    "kmeans = KMeans(n_clusters=n_proto).fit(np.stack(measures.band_1))\n",
    "kmeans_centers = kmeans.cluster_centers_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, center in enumerate(kmeans_centers):\n",
    "    plt.subplot(1, 4, i % 4 + 1)\n",
    "    plt.imshow(center.reshape(75, 75))\n",
    "    if i % 4 == 3:\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "measures.is_iceberg.groupby(kmeans.labels_).apply(pd.value_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Scatter effect\n",
    "\n",
    "---\n",
    "\n",
    "Looking at a graph based method, ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 Preprocessing\n",
    "\n",
    "-jusitfiy \"no feature\" extraction  \n",
    "-and why no smoothing\n",
    "\n",
    "We don't have any particular pre-processing to do, since the data is already nicely formatted. Also since the image are already quite small (75x75), in particular the central zone of interest, we choose not to smooth it (with a gaussian filter by example) in order not to loose important details."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train, validation and test splits\n",
    "\n",
    "later crossval?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# random state is very important as they same split can be used in other notebooks\n",
    "train, valtest = train_test_split(range(len(measures)), test_size=0.3, stratify=measures.is_iceberg, random_state=0)\n",
    "validation, test = train_test_split(valtest, test_size=0.5, stratify=measures.iloc[valtest].is_iceberg, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainval = np.r_[train, validation]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check that we did a fair split for training and testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "measures.iloc[train].is_iceberg.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "measures.iloc[validation].is_iceberg.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "measures.iloc[trainval].is_iceberg.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "measures.iloc[test].is_iceberg.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning and scaling\n",
    "\n",
    "-only angle, among the trials we saw that replace it by median/mean/fixed number  \n",
    "-justify scaling assumption"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "band_scaler = MinMaxScaler()\n",
    "angle_scaler = MinMaxScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for _, e in measures.iloc[train].iterrows():\n",
    "    band_scaler.partial_fit(e.band_1.reshape(1, -1))\n",
    "    band_scaler.partial_fit(e.band_2.reshape(1, -1))\n",
    "    \n",
    "angle_scaler.fit(measures.iloc[train].inc_angle.dropna().values.reshape(-1, 1));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# justify\n",
    "angle_na_replacement = measures.iloc[train].inc_angle.dropna().mean()\n",
    "angle_na_replacement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Usable data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bands = np.stack([\n",
    "    band_scaler.transform(np.stack(measures.band_1)),\n",
    "    band_scaler.transform(np.stack(measures.band_2)),\n",
    "], axis=1).reshape(-1, 2, 75, 75)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "angles = angle_scaler.transform(measures.inc_angle.fillna(angle_na_replacement).values.reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "targets = measures.is_iceberg.values.reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 Graphs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grid\n",
    "Below are different kind of grid that can be used to sample the images.\n",
    "\n",
    "circular"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gcnn import graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Classical 2D grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "small_grid = nx.grid_graph([4, 4, 1])\n",
    "nx.draw(small_grid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2D Grid where each node is connected to the k nearest neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "small_knn = graph.knn(graph.grid_coordinates(5), k=8, metric='cityblock')\n",
    "small_knn = nx.from_numpy_array(small_knn)\n",
    "nx.draw(small_knn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Same as above but with wrap-around"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "small_wraps = graph.kwraps(5, k=1)\n",
    "small_wraps = nx.from_numpy_matrix(small_wraps)        \n",
    "nx.draw(small_wraps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Classical 3D grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "small_grid3d = nx.grid_graph([4, 4, 2])\n",
    "nx.draw(small_grid3d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3D k-nearest-neighbors grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "small_knn3d = graph.knn(\n",
    "    np.r_[\n",
    "        np.c_[graph.grid_coordinates(5), np.zeros(5 * 5)],\n",
    "        np.c_[graph.grid_coordinates(5), np.ones(5 * 5)],\n",
    "    ],\n",
    "    k=16, \n",
    "    metric='cityblock'\n",
    ")\n",
    "small_knn3d = nx.from_numpy_array(small_knn3d)\n",
    "nx.draw(small_knn3d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3D wrap-around grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "small_wraps3d = graph.kwraps3d(5, k=1, d=2)\n",
    "nx.draw(small_wraps3d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "analyze those graph in terms of connectivity etc as in homework 2 using networkx power"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict(small_wraps3d.degree).values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nx.average_node_connectivity(small_grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nx.average_node_connectivity(small_wraps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nx.average_node_connectivity(small_knn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nx.average_node_connectivity(small_grid3d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nx.average_node_connectivity(small_knn3d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "nx.average_node_connectivity(small_wraps3d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Coarsening"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "compare different method in terms of metric we used in homeworks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### graclus\n",
    "graclus implement is courtesy of Michael Defferrard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gcnn.coarsening import graclus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "small_grid = nx.grid_graph([4, 4, 1])\n",
    "small_dist = nx.adjacency_matrix(small_grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graphs, perm = graclus.coarsen(small_dist, levels=3, self_connections=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for g in graphs:\n",
    "    plt.subplot(121)\n",
    "    plt.spy(g.todense())\n",
    "    plt.subplot(122)\n",
    "    nx.draw(nx.from_numpy_array(g.todense()))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### kron reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gcnn.coarsening import kron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "small_grid = nx.grid_graph([8, 8, 1])\n",
    "small_dist = nx.adjacency_matrix(small_grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Gs = kron.graph_multiresolution(sp.sparse.csr_matrix(small_dist), levels=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for g in Gs:\n",
    "    g.set_coordinates()\n",
    "    g.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### algebraic multigrid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gcnn.coarsening import amg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "small_grid = nx.grid_graph([4, 4, 1])\n",
    "small_dist = nx.adjacency_matrix(small_grid)\n",
    "graphs, perm = amg.coarsen_amg(small_dist, levels=3, self_connections=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for g in graphs:\n",
    "    plt.subplot(121)\n",
    "    plt.spy(g.todense())\n",
    "    plt.subplot(122)\n",
    "    nx.draw(nx.from_numpy_array(g.todense()))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### maximum spanning tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gcnn.coarsening import mst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "small_grid = nx.grid_graph([6, 6, 1])\n",
    "small_dist = nx.adjacency_matrix(small_grid)\n",
    "graphs = mst.mst(small_dist.todense(), levels=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for g in graphs:\n",
    "    plt.subplot(121)\n",
    "    plt.spy(g)\n",
    "    plt.subplot(122)\n",
    "    nx.draw(nx.from_numpy_array(g))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the minimum spanning tree and kron based methods, we haven't found an efficient way to know which nodes are being clustered together. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5 Models\n",
    "\n",
    "how to take into account std ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = pd.DataFrame([], columns=['name', 'accuracy', 'precision', 'recall', 'f1']).set_index('name')\n",
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "flat_features = np.c_[bands.reshape(-1, 2 * 75 * 75), angles.reshape(-1, 1)]\n",
    "flat_features.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive methods\n",
    "\n",
    "knn, logistic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.dummy import DummyClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy = DummyClassifier().fit(flat_features[trainval], targets[trainval].ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gcnn.utils import score_classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores.loc['baseline'] = score_classification(targets[test], dummy.predict(flat_features[test]))\n",
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GridSearchCV in associated notebooks for n\n",
    "knn = KNeighborsClassifier(n_neighbors=5).fit(flat_features[trainval], targets[trainval].ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores.loc['knn'] = score_classification(targets[test], knn.predict(flat_features[test]))\n",
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GridSearchCV in associated notebooks for C\n",
    "logistic = LogisticRegression(C=1).fit(flat_features[trainval], targets[trainval].ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores.loc['logistic'] = score_classification(targets[test], logistic.predict(flat_features[test]))\n",
    "scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convolution baseline\n",
    "\n",
    "say which, why LeNet (paper), gridsearch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "integrate angle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cuda = torch.cuda.is_available()\n",
    "cuda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.utils.data as data_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bands_t = torch.from_numpy(bands).float()\n",
    "angles_t = torch.from_numpy(angles).float()\n",
    "targets_t = torch.from_numpy(targets).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gcnn.nets import BaselineCNN\n",
    "from skorch import NeuralNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn = NeuralNet(\n",
    "    BaselineCNN,\n",
    "    optimizer=torch.optim.Adam,\n",
    "    criterion=nn.BCEWithLogitsLoss, \n",
    "    max_epochs=15,\n",
    "    batch_size=50,\n",
    "    lr=0.001,\n",
    "    use_cuda=cuda\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn.partial_fit(dict(x=bands_t[train], x2=angles_t[train]), targets_t[train])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gcnn.utils import sigmoid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_predict = sigmoid(cnn.predict_proba(dict(x=bands_t[test], x2=angles_t[test]))).round()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores.loc['conv'] = score_classification(targets[test], cnn_predict)\n",
    "scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Graph Convolution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fourier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gcnn.coarsening import graclus\n",
    "import scipy.sparse\n",
    "from scipy.sparse import csgraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gc_knn_laps = graph.knn(graph.grid_coordinates(75), k=8, metric='cityblock') > 0\n",
    "# courtesy of Michael Defferrard\n",
    "gc_knn_laps, gc_knn_perm = graclus.coarsen(sp.sparse.csr.csr_matrix(gc_knn_laps), levels=3, self_connections=False)\n",
    "gc_knn_laps = [csgraph.laplacian(g, normed=True) for g in gc_knn_laps[:-1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gcnn.layers import GraphChebyConv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Temp(nn.Module):\n",
    "    \n",
    "    def __init__(self, conv1_dim=32, conv2_dim=64):\n",
    "        super().__init__()\n",
    "        \n",
    "        l0 = torch.from_numpy(gc_knn_laps[0].todense()).float()\n",
    "        l2 = torch.from_numpy(gc_knn_laps[2].todense()).float()\n",
    "\n",
    "        self.gc1 = nn.Sequential(\n",
    "            # GraphFourierConv(l0.cuda() if cuda else l0, 1, conv1_dim, bias=True),\n",
    "            GraphChebyConv(l0.cuda() if cuda else l0, 1, conv1_dim, 25, bias=True),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool1d(4),\n",
    "        )\n",
    "\n",
    "        self.gc2 = nn.Sequential(\n",
    "            # GraphFourierConv(l2.cuda() if cuda else l2, conv1_dim, conv2_dim, bias=True),\n",
    "            GraphChebyConv(l2.cuda() if cuda else l2, conv1_dim, conv2_dim, 25, bias=True),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool1d(4),\n",
    "        )\n",
    "\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(len(l2) // 4 * conv2_dim, 512),\n",
    "            nn.ReLU(),\n",
    "            # nn.Dropout(0.5),\n",
    "            nn.Linear(512, 1),\n",
    "        )\n",
    "\n",
    "    def forward(self, x, x2):\n",
    "        out = self.gc1(x)\n",
    "        out = self.gc2(out)\n",
    "        out = out.view(out.size(0), -1)\n",
    "        out = self.fc(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bands_tg = bands[:, 0, :, :].reshape(-1, 75 * 75)\n",
    "bands_tg = graclus.perm_data(bands_tg, gc_knn_perm)\n",
    "bands_tg = torch.from_numpy(bands_tg).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gcnn_knn = NeuralNet(\n",
    "    Temp,\n",
    "    optimizer=torch.optim.Adam,\n",
    "    criterion=nn.BCEWithLogitsLoss, \n",
    "    max_epochs=15,\n",
    "    batch_size=50,\n",
    "    lr=0.001,\n",
    "    use_cuda=cuda\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gcnn_knn.partial_fit(dict(x=bands_tg[train], x2=angles_t[train]), targets_t[train])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gcnn_knn_predict = sigmoid(gcnn_knn.predict_proba(dict(x=bands_tg[test], x2=angles_t[test]))).round()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores.loc['gcnn_knn'] = score_classification(targets[test], gcnn_knn_predict)\n",
    "scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "chebyshev"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Graph convolution with amg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gcnn.coarsening import amg\n",
    "import scipy.sparse\n",
    "from scipy.sparse import csgraph\n",
    "import skimage.transform as transform\n",
    "from scipy import spatial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "gc_knn_laps = graph.knn(graph.grid_coordinates(38), k=4, metric='cityblock') > 0\n",
    "print(gc_knn_laps.shape)\n",
    "image = transform.rescale(measures.iloc[5].band_1.reshape(75, 75), 0.5, mode='constant')\n",
    "image_grid = spatial.distance.squareform(spatial.distance.pdist(image.reshape(-1, 1), metric='euclidean'))\n",
    "image_grid[np.where(gc_knn_laps == 0)] = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "gc_knn_laps = graph.knn(graph.grid_coordinates(75), k=8, metric='cityblock') > 0a\n",
    "gc_knn_laps, gc_knn_perm = amg.coarsen_amg(sp.sparse.csr.csr_matrix(gc_knn_laps), levels=3)\n",
    "gc_knn_laps = [csgraph.laplacian(g, normed=True) for g in gc_knn_laps[:-1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bands_tg = bands[:, 0, :, :].reshape(-1, 75 * 75)\n",
    "bands_tg = amg.perm_data(bands_tg, gc_knn_perm)\n",
    "bands_tg = torch.from_numpy(bands_tg).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gcnn_knn = NeuralNet(\n",
    "    Temp,\n",
    "    optimizer=torch.optim.Adam,\n",
    "    criterion=nn.BCEWithLogitsLoss, \n",
    "    max_epochs=15,\n",
    "    batch_size=50,\n",
    "    lr=0.001,\n",
    "    use_cuda=cuda\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gcnn_knn.partial_fit(dict(x=bands_tg[train], x2=angles_t[train]), targets_t[train])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gcnn_knn_predict = sigmoid(gcnn_knn.predict_proba(dict(x=bands_tg[test], x2=angles_t[test]))).round()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores.loc['gcnn_knn'] = score_classification(targets[test], gcnn_knn_predict)\n",
    "scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pooling\n",
    "\n",
    "pattern-based"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6 Evaluation\n",
    "\n",
    "### Models\n",
    "\n",
    "### Stacking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7 Conclusion\n",
    "\n",
    "### Improvements\n",
    "\n",
    "- sparse operations\n",
    "- speed\n",
    "\n",
    "### Acknowledgements"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8 References\n",
    "\n",
    "- TORRES, Ramon, SNOEIJ, Paul, GEUDTNER, Dirk, et al. GMES Sentinel-1 mission. Remote Sensing of Environment, 2012, vol. 120, p. 9-24.\n",
    "- SHUMAN, David I., NARANG, Sunil K., FROSSARD, Pascal, et al. The emerging field of signal processing on graphs: Extending high-dimensional data analysis to networks and other irregular domains. IEEE Signal Processing Magazine, 2013, vol. 30, no 3, p. 83-98.\n",
    "- BRONSTEIN, Michael M., BRUNA, Joan, LECUN, Yann, et al. Geometric deep learning: going beyond euclidean data. IEEE Signal Processing Magazine, 2017, vol. 34, no 4, p. 18-42.\n",
    "- DEFFERRARD, MichaÃ«l, BRESSON, Xavier, et VANDERGHEYNST, Pierre. Convolutional neural networks on graphs with fast localized spectral filtering. In : Advances in Neural Information Processing Systems. 2016. p. 3844-3852.\n",
    "- NGUYEN Ha Q., DO Minh N, et al. Downsampling of Signal on Graphs Via Maximum Spanning Trees. IEEE Transactions on Signal Processing, 2015, vol. 63, no 1.\n",
    "- DORFLER Florain, BULLO Francesco. Kron reduction of graphs with applications to electrical networks. 2011."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
